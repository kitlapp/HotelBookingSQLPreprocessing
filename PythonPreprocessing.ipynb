{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTWnAEiRgwz0CZHh0wtIsV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kitlapp/HotelBookingSQLPreprocessing/blob/kimon/PythonPreprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hotel Booking Data Preprocessing in the Google Colab Platform\n",
        "\n",
        "The goal of this project is to migrate the LookerStudioKPIDashboard project, originally prepared in a local environment, to the cloud, specifically to Google Colab. This notebook therefore includes all Python scripts required to create a cleaned DataFrame, suitable for KPI calculations for booking cancellation monitoring and further BI processing.\n",
        "\n",
        "The preprocessing in this approach is slightly different from the one used for machine learning purposes (BookingCancellationPrediction). For example, null values must be handled in both DataFrames, since BI tools cannot work with them either. However, in cases such as dates, the KPI-focused DataFrame does not require trigonometric component calculations to encode cyclical patterns. Instead, it only needs dates in the correct format, e.g., \"YYYY/MM/DD\".\n",
        "\n",
        "The advantages of this migration are:\n",
        "\n",
        "1. A local environment setup is not required (e.g., connecting to a Jupyter server, managing the command line through Conda, keeping command history logs for reproducible setup, or using Git Bash for version control). Even a gitignore file is unnecessary, since one can carefully choose what to upload directly to this specific GitHub repository from their PC.\n",
        "2. Working in Google Colab and potentially purchasing resources in the future makes this approach scalable.\n",
        "3. This environment can be shared and promote collaboration more easily, because replicating the project on any machine only requires cloning the GitHub repository URL in Google Colab. From there, one can immediately run the scripts or start enhancing the code.\n",
        "4. Google Colab has Gemini integrated which enhances coding help.\n",
        "\n"
      ],
      "metadata": {
        "id": "b6vcC-7-I7Km"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Importing Libraries and Reading the Raw Data from Google Drive"
      ],
      "metadata": {
        "id": "zn7qbst5P9AT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "16Ag9xrFV6vF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8e7eac1-6d1a-4bc6-dc19-5090c83dfd30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "(119390, 36)\n",
            "Number of Duplicates: 0\n"
          ]
        }
      ],
      "source": [
        "# Authorize access of Google Colab to Google Drive\n",
        "# The code below has to be executed only once\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Google Drive path to the dataset\n",
        "filepath = '/content/drive/MyDrive/PyCharm_Projects/hotel_booking_RAW.csv'\n",
        "\n",
        "# Read csv file to a DataFrame\n",
        "df_raw = pd.read_csv(filepath)\n",
        "\n",
        "# Rows and columns check of raw data (For SQL preprocessing comparison)\n",
        "print(df_raw.shape)\n",
        "\n",
        "# Check for duplicates\n",
        "print('Number of Duplicates:', df_raw.duplicated().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Preprocessing"
      ],
      "metadata": {
        "id": "aR3ZG8tnV2VW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. Handling Null Values"
      ],
      "metadata": {
        "id": "xLEqxSSaV535"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for nulls\n",
        "df_raw.isna().sum()"
      ],
      "metadata": {
        "id": "ESTTNuZw1Yt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a copy before further preprocessing ('dash': Dashboard, to link with the purpose of this preprocessing)\n",
        "dfdash2 = df_raw.copy()\n",
        "\n",
        "# Fill missing values in 'children' column with the most frequent value (mode)\n",
        "dfdash2['children'] = dfdash2['children'].fillna(value=dfdash2['children'].mode()[0])\n",
        "\n",
        "# Replace missing values in 'agent' with 0, indicating direct bookings without a travel agent\n",
        "dfdash2['agent'] = dfdash2['agent'].fillna(value=0)\n",
        "\n",
        "# Replace missing values in 'company' with 0, meaning bookings not linked to any company\n",
        "dfdash2['company'] = dfdash2['company'].fillna(value=0)\n",
        "\n",
        "# Drop all rows with missing 'country' values since location info is important for analysis\n",
        "dfdash2 = dfdash2.drop(labels=dfdash2.loc[dfdash2['country'].isna()].index)\n",
        "\n",
        "# Rows and columns check of dfdash2 (For SQL preprocessing comparison)\n",
        "print(dfdash2.shape)"
      ],
      "metadata": {
        "id": "P6DxKKETxOh5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d301b5a-c402-4179-93df-2e6d24471faa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(118902, 36)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2. Handling Date-Related Columns"
      ],
      "metadata": {
        "id": "pc5loKBEdsuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a copy before further preprocessing\n",
        "dfdash3 = dfdash2.copy()\n",
        "\n",
        "# Create a dictionary to convert month names to their corresponding numeric values (as strings)\n",
        "month_mapping = {\n",
        "    \"January\": '1', \"February\": '2', \"March\": '3', \"April\": '4', \"May\": '5',\n",
        "    \"June\": '6', \"July\": '7', \"August\": '8', \"September\": '9', \"October\": '10',\n",
        "    \"November\": '11', \"December\": '12'\n",
        "}\n",
        "\n",
        "# Map month names to integers using the same dictionary\n",
        "dfdash3['arrival_date_month'] = dfdash3['arrival_date_month'].map(month_mapping).astype(int)\n",
        "\n",
        "# Combine year, month, and day columns into a single date string in 'YYYY-MM-DD' format\n",
        "dfdash3['arrival_date'] = (\n",
        "    dfdash3['arrival_date_year'].astype(str) + '-' +\n",
        "    dfdash3['arrival_date_month'].astype(str) + '-' +\n",
        "    dfdash3['arrival_date_day_of_month'].astype(str)\n",
        ")\n",
        "\n",
        "# Convert the date strings into proper datetime objects\n",
        "# for easier time-based analysis\n",
        "dfdash3['arrival_date'] = pd.to_datetime(dfdash3['arrival_date'], format='%Y-%m-%d')\n",
        "\n",
        "# Rows and columns check of dfdash3 (For SQL preprocessing comparison)\n",
        "print(dfdash3.shape)"
      ],
      "metadata": {
        "id": "VfNuuRdDW90m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7137aa37-6688-4ed2-c353-d84f07911a2e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(118902, 37)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3. Dropping Unimportant Columns"
      ],
      "metadata": {
        "id": "dQTTXEQiqfkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of columns to be dropped from the dashboard dataframe\n",
        "dashcols_to_be_dropped = ['name', 'email', 'arrival_date_month', 'arrival_date_day_of_month', 'phone-number',\n",
        "                          'credit_card', 'reservation_status', 'reservation_status_date', 'assigned_room_type',\n",
        "                          'deposit_type', 'required_car_parking_spaces', 'arrival_date_week_number']\n",
        "\n",
        "# Make a copy before further preprocessing\n",
        "dfdash4 = dfdash3.copy()\n",
        "\n",
        "# Drop the unimportant columns from the dashboard dataframe\n",
        "dfdash4 = dfdash4.drop(columns=dashcols_to_be_dropped)\n",
        "\n",
        "# Rows and columns check of dfdash4 (For SQL preprocessing comparison)\n",
        "print(dfdash4.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUviDTfIqn00",
        "outputId": "6c5a5a00-d6db-49e4-9696-833df0cc3630"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(118902, 25)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4. Creating total_kids Column"
      ],
      "metadata": {
        "id": "m2WDTZF9xwft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a copy before further preprocessing\n",
        "dfdash5 = dfdash4.copy()\n",
        "\n",
        "# Merge the 'children' and 'babies' columns to create a new column 'total_kids' representing the total number of kids\n",
        "dfdash5['total_kids'] = dfdash5['children'].astype(int) + dfdash5['babies'].astype(int)\n",
        "\n",
        "# Drop the original 'children' and 'babies' columns after merging\n",
        "dfdash5 = dfdash5.drop(columns=['children', 'babies'])\n",
        "\n",
        "# Drop rows with outliers (total kids > 3) and reset index in the dashboard dataframe\n",
        "dfdash5 = dfdash5.loc[dfdash5['total_kids'] <= 3].reset_index(drop=True)\n",
        "\n",
        "# Rows and columns check of dfdash5 (For SQL preprocessing comparison)\n",
        "print(dfdash5.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZL6K9fk4x66s",
        "outputId": "886ae381-c45d-40ee-ae35-69e57c0a686e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(118899, 24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5. Handling adults Column\n",
        "\n",
        "There are observations where both adults and total_kids equal 0. This can't be explained and therefore all rows where adults = 0 will be dropped. Additionally, in all cases where adults were greater than 4 the bookings were canceled and the adr equals 0. For this reason, values for adults from 1 to 4 are considered the most explainable and normal. We will drop all other values."
      ],
      "metadata": {
        "id": "OXcUjNVA1AlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a copy before further preprocessing\n",
        "dfdash6 = dfdash5.copy()\n",
        "\n",
        "# Exclude bookings where the number of adults is 0. Also, ensure that the number of adults is between 1 and 4.\n",
        "dfdash6 = dfdash6[(dfdash6['adults'] > 0) & (dfdash6['adults'] <= 4)].reset_index(drop=True)\n",
        "\n",
        "# Rows and columns check of dfdash6 (For SQL preprocessing comparison)\n",
        "print(dfdash6.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hossJPdb1LmN",
        "outputId": "0b7969a2-41a0-4993-8d0c-605be04f8209"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(118490, 24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.6. Handling meal Column"
      ],
      "metadata": {
        "id": "XQM9TSbP205m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a copy before further preprocessing\n",
        "dfdash7 = dfdash6.copy()\n",
        "\n",
        "# Create a dictionary to map meal types to numerical values\n",
        "meal_mapping = {'BB': 1, 'HB': 2, 'SC': 0, 'FB': 3}\n",
        "\n",
        "# Drop rows where the 'meal' column is 'Undefined', indicating no meal choice\n",
        "dfdash7 = dfdash7.drop(labels=dfdash7[dfdash7['meal'] == 'Undefined'].index).reset_index(drop=True)\n",
        "\n",
        "# Rename the 'meal' column to 'number_of_meals' for clarity\n",
        "dfdash7 = dfdash7.rename(columns={'meal': 'number_of_meals'})\n",
        "\n",
        "# Map the dictionary to the 'number_of_meals' column, reducing complexity\n",
        "dfdash7['number_of_meals'] = dfdash7['number_of_meals'].map(meal_mapping).astype(int)\n",
        "\n",
        "# Rows and columns check of dfdash7 (For SQL preprocessing comparison)\n",
        "print(dfdash7.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46A1KtXJ231I",
        "outputId": "89355f5b-e925-4888-b3f4-57c0ba1777fa"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(117325, 24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.7. Handling market_segment Column"
      ],
      "metadata": {
        "id": "ncRl8bUi57wk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a copy before further preprocessing\n",
        "dfdash8 = dfdash7.copy()\n",
        "\n",
        "# Drop all rows where the 'market_segment' column has the category 'Undefined'\n",
        "dfdash8 = dfdash8.drop(labels=dfdash8[dfdash8['market_segment'] == 'Undefined'].index).reset_index(drop=True)\n",
        "\n",
        "# Replace the 'Complementary' and 'Aviation' categories in the 'market_segment' column with 'Other'\n",
        "dfdash8['market_segment'] = dfdash8['market_segment'].replace(\n",
        "    {'Complementary': 'Other', 'Aviation': 'Other'}).astype('category')\n",
        "\n",
        "# Rows and columns check of dfdash8 (For SQL preprocessing comparison)\n",
        "print(dfdash8.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysFfKtTa5_lD",
        "outputId": "d9b98dd3-dd98-4491-bae4-cfcab55a3101"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(117323, 24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.8. Handling Distribution Channel Column"
      ],
      "metadata": {
        "id": "b4RMllh-7lbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a copy before further preprocessing\n",
        "dfdash9 = dfdash8.copy()\n",
        "\n",
        "# Drop all rows where the 'distribution_channel' column has the category 'Undefined'\n",
        "dfdash9 = dfdash9.drop(labels=dfdash9[dfdash9['distribution_channel'] == 'Undefined'].index).reset_index(drop=True)\n",
        "\n",
        "# Convert the 'distribution_channel' column to categorical type\n",
        "dfdash9['distribution_channel'] = dfdash9['distribution_channel'].astype('category')\n",
        "\n",
        "# Rows and columns check of dfdash9 (For SQL preprocessing comparison)\n",
        "print(dfdash9.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOeqfOog7ocn",
        "outputId": "2b82ec8f-9208-45dd-c33d-a52d98c27fb8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(117320, 24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.9. Handling reserved_room_type Column"
      ],
      "metadata": {
        "id": "T5utT25WBTgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a copy before further preprocessing\n",
        "dfdash10 = dfdash9.copy()\n",
        "\n",
        "# Merge categories in the 'reserved_room_type' column, combining multiple categories into 'Other'\n",
        "dfdash10['reserved_room_type'] = dfdash10['reserved_room_type'].replace(\n",
        "    {'C': 'Other', 'B': 'Other', 'H': 'Other', 'L': 'Other'}).astype('category')\n",
        "\n",
        "# Rows and columns check of dfdash9 (For SQL preprocessing comparison)\n",
        "print(dfdash10.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQo3mdwgBbP5",
        "outputId": "616b4bbe-3cdf-4c95-d6c6-bedf62798ff6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(117320, 24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.10. Handling agent & company Columns"
      ],
      "metadata": {
        "id": "UleunzgfExta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a copy before further preprocessing\n",
        "dfdash11 = dfdash10.copy()\n",
        "\n",
        "# Convert 'agent' column to binary: 1 if not 0, else 0\n",
        "dfdash11['agent'] = dfdash11['agent'].apply(lambda x: 1 if x != 0 else 0)\n",
        "\n",
        "# Convert 'company' column to binary: 1 if not 0, else 0\n",
        "dfdash11['company'] = dfdash11['company'].apply(lambda x: 1 if x != 0 else 0)\n",
        "\n",
        "# Rename the columns to more intuitive names\n",
        "dfdash11 = dfdash11.rename(columns={'agent': 'has_agent', 'company': 'has_company'})\n",
        "\n",
        "# Rows and columns check of dfdash9 (For SQL preprocessing comparison)\n",
        "print(dfdash11.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHTVFZASE29Q",
        "outputId": "c911d52c-ece6-4080-b627-2c5b262068c3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(117320, 24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.11. Handling adr & lead_time Outliers"
      ],
      "metadata": {
        "id": "fPzB9KYIHOPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a copy before further preprocessing\n",
        "dfdash12 = dfdash11.copy()\n",
        "\n",
        "# Set the threshold values for ADR (average daily rate) and lead_time outliers:\n",
        "adr_outlier_value = 5400  # Maximum acceptable value for ADR\n",
        "lead_time_outlier_border = 640  # Maximum acceptable value for lead time\n",
        "\n",
        "# Remove rows where ADR is higher than the defined threshold or negative\n",
        "dfdash12 = dfdash12.loc[(dfdash12['adr'] < adr_outlier_value) & (dfdash12['adr'] >= 0)].reset_index(drop=True)\n",
        "\n",
        "# Remove rows where lead_time exceeds the defined threshold in the dashboard dataframe\n",
        "dfdash12 = dfdash12.loc[dfdash12['lead_time'] < lead_time_outlier_border].reset_index(drop=True)\n",
        "\n",
        "# Rows and columns check of dfdash12 (For SQL preprocessing comparison)\n",
        "print(dfdash12.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EHdT8EcE_4u",
        "outputId": "b3340acf-5a72-4b8f-d85b-b5cc0c6a6f62"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(117316, 24)\n"
          ]
        }
      ]
    }
  ]
}