{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMl0kd0m3DcFn+FR/1D4hDG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kitlapp/HotelBookingSQLPreprocessing/blob/kimon/PythonPreprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hotel Booking Data Preprocessing in the Google Colab Platform\n",
        "\n",
        "The goal of this project is to migrate the LookerStudioKPIDashboard project, originally prepared in a local environment, to the cloud, specifically to Google Colab. This notebook therefore includes all Python scripts required to create a cleaned DataFrame, suitable for KPI calculations for booking cancellation monitoring and further BI processing.\n",
        "\n",
        "The preprocessing in this approach is slightly different from the one used for machine learning purposes (BookingCancellationPrediction). For example, null values must be handled in both DataFrames, since BI tools cannot work with them either. However, in cases such as dates, the KPI-focused DataFrame does not require trigonometric component calculations to encode cyclical patterns. Instead, it only needs dates in the correct format, e.g., \"YYYY/MM/DD\".\n",
        "\n",
        "The advantages of this migration are:\n",
        "\n",
        "1. A local environment setup is not required (e.g., connecting to a Jupyter server, managing the command line through Conda, keeping command history logs for reproducible setup, or using Git Bash for version control). Even a gitignore file is unnecessary, since one can carefully choose what to upload directly to this specific GitHub repository from their PC.\n",
        "2. Working in Google Colab and potentially purchasing resources in the future makes this approach scalable.\n",
        "3. This environment can be shared and promote collaboration more easily, because replicating the project on any machine only requires cloning the GitHub repository URL in Google Colab. From there, one can immediately run the scripts or start enhancing the code.\n",
        "4. Google Colab has Gemini integrated which enhances coding help.\n",
        "\n"
      ],
      "metadata": {
        "id": "b6vcC-7-I7Km"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Importing Libraries and Reading the Raw Data from Google Drive"
      ],
      "metadata": {
        "id": "zn7qbst5P9AT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "16Ag9xrFV6vF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8e7eac1-6d1a-4bc6-dc19-5090c83dfd30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "(119390, 36)\n",
            "Number of Duplicates: 0\n"
          ]
        }
      ],
      "source": [
        "# Authorize access of Google Colab to Google Drive\n",
        "# The code below has to be executed only once\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Google Drive path to the dataset\n",
        "filepath = '/content/drive/MyDrive/PyCharm_Projects/hotel_booking_RAW.csv'\n",
        "\n",
        "# Read csv file to a DataFrame\n",
        "df_raw = pd.read_csv(filepath)\n",
        "\n",
        "# Rows and columns check of raw data (For SQL preprocessing comparison)\n",
        "print(df_raw.shape)\n",
        "\n",
        "# Check for duplicates\n",
        "print('Number of Duplicates:', df_raw.duplicated().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Preprocessing"
      ],
      "metadata": {
        "id": "aR3ZG8tnV2VW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. Handling Null Values"
      ],
      "metadata": {
        "id": "xLEqxSSaV535"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for nulls\n",
        "df_raw.isna().sum()"
      ],
      "metadata": {
        "id": "ESTTNuZw1Yt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a copy before further preprocessing ('dash': Dashboard, to link with the purpose of this preprocessing)\n",
        "dfdash2 = df_raw.copy()\n",
        "\n",
        "# Fill missing values in 'children' column with the most frequent value (mode)\n",
        "dfdash2['children'] = dfdash2['children'].fillna(value=dfdash2['children'].mode()[0])\n",
        "\n",
        "# Replace missing values in 'agent' with 0, indicating direct bookings without a travel agent\n",
        "dfdash2['agent'] = dfdash2['agent'].fillna(value=0)\n",
        "\n",
        "# Replace missing values in 'company' with 0, meaning bookings not linked to any company\n",
        "dfdash2['company'] = dfdash2['company'].fillna(value=0)\n",
        "\n",
        "# Drop all rows with missing 'country' values since location info is important for analysis\n",
        "dfdash2 = dfdash2.drop(labels=dfdash2.loc[dfdash2['country'].isna()].index)\n",
        "\n",
        "# Rows and columns check of dfdash2 (For SQL preprocessing comparison)\n",
        "print(dfdash2.shape)"
      ],
      "metadata": {
        "id": "P6DxKKETxOh5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d301b5a-c402-4179-93df-2e6d24471faa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(118902, 36)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2. Handling Date-Related Columns"
      ],
      "metadata": {
        "id": "pc5loKBEdsuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a copy before further preprocessing\n",
        "dfdash3 = dfdash2.copy()\n",
        "\n",
        "# Create a dictionary to convert month names to their corresponding numeric values (as strings)\n",
        "month_mapping = {\n",
        "    \"January\": '1', \"February\": '2', \"March\": '3', \"April\": '4', \"May\": '5',\n",
        "    \"June\": '6', \"July\": '7', \"August\": '8', \"September\": '9', \"October\": '10',\n",
        "    \"November\": '11', \"December\": '12'\n",
        "}\n",
        "\n",
        "# Map month names to integers using the same dictionary\n",
        "dfdash3['arrival_date_month'] = dfdash3['arrival_date_month'].map(month_mapping).astype(int)\n",
        "\n",
        "# Combine year, month, and day columns into a single date string in 'YYYY-MM-DD' format\n",
        "dfdash3['arrival_date'] = (\n",
        "    dfdash3['arrival_date_year'].astype(str) + '-' +\n",
        "    dfdash3['arrival_date_month'].astype(str) + '-' +\n",
        "    dfdash3['arrival_date_day_of_month'].astype(str)\n",
        ")\n",
        "\n",
        "# Convert the date strings into proper datetime objects\n",
        "# for easier time-based analysis\n",
        "dfdash3['arrival_date'] = pd.to_datetime(dfdash3['arrival_date'], format='%Y-%m-%d')\n",
        "\n",
        "# Rows and columns check of dfdash3 (For SQL preprocessing comparison)\n",
        "print(dfdash3.shape)"
      ],
      "metadata": {
        "id": "VfNuuRdDW90m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7137aa37-6688-4ed2-c353-d84f07911a2e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(118902, 37)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3. Dropping Unimportant Columns"
      ],
      "metadata": {
        "id": "dQTTXEQiqfkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of columns to be dropped from the dashboard dataframe\n",
        "dashcols_to_be_dropped = ['name', 'email', 'arrival_date_month', 'arrival_date_day_of_month', 'phone-number',\n",
        "                          'credit_card', 'reservation_status', 'reservation_status_date', 'assigned_room_type',\n",
        "                          'deposit_type', 'required_car_parking_spaces', 'arrival_date_week_number']\n",
        "\n",
        "# Create a copy of the dashboard dataframe for further processing\n",
        "dfdash4 = dfdash3.copy()\n",
        "\n",
        "# Drop the unimportant columns from the dashboard dataframe\n",
        "dfdash4 = dfdash4.drop(columns=dashcols_to_be_dropped)\n",
        "\n",
        "# Rows and columns check of dfdash4 (For SQL preprocessing comparison)\n",
        "print(dfdash4.shape)\n"
      ],
      "metadata": {
        "id": "jUviDTfIqn00",
        "outputId": "6c5a5a00-d6db-49e4-9696-833df0cc3630",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(118902, 25)\n"
          ]
        }
      ]
    }
  ]
}